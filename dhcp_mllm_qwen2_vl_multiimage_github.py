# -*- coding: utf-8 -*-
"""dhcp_mllm_qwen2_VL_multiimage_github.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pptez4qIrghdbfmvAuVI2FN8QQxTvqBG

Preprocessing, Labeling and Training
"""

# -*- coding: utf-8 -*-
"""dhcp_mllm_qwen2_VL.ipynb


## **Predicting gestational age at scan and birth for neonates from the developing Human Connectome Project**


## **The Data**

The cerebral cortex is a thin layer of tissue at the outer surface of the brain, where most cells responsible for higher-order cognitive processing are found. The creation and maturation of these cells develops rapidly during fetal and early neonatal development (in the weeks and months just before and after birth), which leads to cortical folding and dramatic changes in the observed intensities of T1 and T2-weighted Magnetic Resonance Imaging (MRI). In this exercise you will use different MRI-derived metrics of cortical maturation to **predict the gestational age at scan _and_ at birth of babies scanned as part of the Developing Human Connectome Project** (dHCP, Makropoulos et al 2018).

We will use metrics of cortical thickness, cortical curvature, cortical myelination and sulcal depth generated from mesh models of the cortical surface. While we will explore Geometric Deep Learning (for direct analysis of surface imaging data) in the last lecture of  course. Right now you will work with files which have been projected to 2D via the sphere:


## **Getting started**

First mount your Google Drive and import all necessary packages
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
!pip3 install SimpleITK
!pip3 install imageio
!pip3 install scikit-image
!pip3 install seaborn


import SimpleITK as sitk
import time
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset,DataLoader
import torch.nn as nn
import torch.optim as optim
import os
import numpy as np
import torchvision
# import scipy.misc # scipy.misc is deprecated
import imageio
import pickle
import matplotlib.pyplot as plt
import pandas as pd
import nibabel as nib
from torch.utils.data.sampler import SubsetRandomSampler
from skimage.transform import resize # skimage.transform.resize might not be used if original size is kept
from torchvision import transforms, utils
from tqdm import tqdm

import time
import seaborn as sns

# # initialize network weights as a gaussian
# def weights_init(m):
#     classname = m.__class__.__name__
#     if classname.find('Conv') != -1:
#         m.weight.data.normal_(0.0, 0.02)
#     elif classname.find('BatchNorm') != -1:
#         m.weight.data.normal_(1.0, 0.02)
#         m.bias.data.fill_(0)

save_path = 'results'

if not os.path.exists(save_path):
    os.makedirs(save_path)

# It's recommended to run this in a separate cell at the beginning of your notebook
!pip install transformers>=4.40.0
!pip install torch torchvision torchaudio
!pip install accelerate
!pip install peft
!pip install bitsandbytes
!pip install tiktoken
!pip install sentencepiece # Often a dependency for tokenizers

# It's highly recommanded to use `[decord]` feature for faster video loading.
!pip install qwen-vl-utils[decord]==0.0.8

"""## **Accessing and loading the data set**

The dHCP dataset is publicly available to researchers upon application. More information can be found at http://www.developingconnectome.org/project/.
"""

### cell/for testing file input/output/visualisation

dirname='/content/drive/MyDrive/data-2022-pma'

example_im=np.load(os.path.join(dirname,'2D_projection_L_sub-0.npy'))


meta_path='/content/drive/MyDrive/meta_2022_pma.pkl'
label_df=pd.read_pickle(meta_path)

plt.imshow(example_im[:,:,0]) # Show first channel

print('subject meta data', label_df.iloc[0])
print('example shape ', example_im.shape) # Should be (H, W, C) e.g. (240, 320, 4)

# NOTE PyTorch is going to expect your data batches to have shape BxCxHxW
# (Batch,Channels, Height, Width)!

"""

Complete a custom Dataset for this exercise. Make sure to return both images and their corresponding labels and return image tensors of the correct shape.
"""


#we create a class that inherits the torch Dataset abstract class
class BrainClassificationDataset(torch.utils.data.Dataset):

    # initialise the class based on the folder containing the data and the project dataframe
    def __init__(self, folder='', meta='',transform=None):

        self.dirname = folder
        self.meta = meta
        self.transform = transform # Transform is usually applied to PIL images or tensors

    def __len__(self):

        return len(self.meta)

    def __getitem__(self, idx):


        meta_sample=self.meta.iloc[idx] # this will return the idx-th row from the dataframe
        # Corrected subject ID formatting to ensure it's an integer before string conversion if needed
        subj_id_str = str(int(meta_sample['subj_id']))
        image_name = os.path.join(self.dirname,'2D_projection_L_sub-' + subj_id_str + '.npy')
        # label_name = self.meta['subj_id'][idx].astype(int) # This was likely for a different task, label is scan_pma


        image = np.load(image_name) # Expected shape (H, W, C) e.g. (240, 320, 4)
        label = self.meta['scan_pma'][idx] * 7


        img_tensor = torch.from_numpy(image).permute(2,0,1).to(torch.float) # C, H, W
        lab_tensor = torch.from_numpy(np.array(label)).to(torch.float)


        lab_tensor = lab_tensor.unsqueeze(0) # Make it [1]

        # Sample does not need transformation here if it's handled later or if transform applies to np array
        sample = img_tensor, lab_tensor

        if self.transform: # If a transform is defined (e.g. for augmentation)
             # Typical torchvision transforms expect PIL Image or tensor.
             # If img_tensor is already a tensor, ensure transform is compatible.
             # For this coursework, transforms might not be heavily used with direct numpy loading to tensor.
             # If transforms were for PIL, conversion would be:
             # pil_img = Image.fromarray(image_channel_data) -> transform(pil_img) -> ToTensor()
             # As we are directly making tensors, self.transform might be for tensor-based augmentations.
             # The original coursework context with ResNet might have had different transform needs.
             # For Qwen-VL, we will convert to PIL later anyway.
             # Let's assume transform is None or compatible with tensor inputs for now.
             # If self.transform expects a tuple (img, label), it needs to be handled carefully.
             # Often, transform is applied only to the image.
             # E.g. sample = self.transform(img_tensor), lab_tensor
             # Given the original template, it implies transform takes the tuple.
            sample = self.transform(sample)


        return sample




ds = BrainClassificationDataset(dirname,label_df, transform=None) # transform is None for now
# then we create the dataset


print(f"Dataset length: {len(ds)}") # Using f-string for clarity
shuffle_dataset = True
random_seed= 42

# Creating data indices for training and validation splits:
dataset_size = len(ds)
indices = list(range(dataset_size))

# split = -51 # Takes the last 51 for validation
num_validation_samples = 51
# Ensure split index is calculated correctly if dataset size is small
if dataset_size <= num_validation_samples:
    raise ValueError("Dataset size is too small for the specified validation split.")
split_idx = dataset_size - num_validation_samples


if shuffle_dataset :
    np.random.seed(random_seed)
    np.random.shuffle(indices)

# Corrected splitting logic:
train_indices = indices[:split_idx]
val_indices = indices[split_idx:]

# Creating data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)

# Batch sizes for Qwen-VL fine-tuning should be small, e.g., 1, 2, or 4, depending on GPU VRAM
# The original coursework batch sizes (32, 51) are for smaller CNNs.
# Let's use a smaller batch size for train_loader for demonstration with Qwen.
# Validation can use a larger batch if memory allows for faster eval, but inference is often done one by one or small batches.
BATCH_SIZE_TRAIN_QWEN = 1 # Adjust based on your GPU memory for Qwen
BATCH_SIZE_VAL_QWEN = 1   # Adjust based on your GPU memory for Qwen

train_loader = torch.utils.data.DataLoader(ds, batch_size=BATCH_SIZE_TRAIN_QWEN,
                                           sampler=train_sampler)
validation_loader = torch.utils.data.DataLoader(ds, batch_size=BATCH_SIZE_VAL_QWEN, # Or len(val_indices) if memory permits and processing is batched
                                                sampler=valid_sampler)


# return one batch from the train dataloader; print shape and plot
if len(train_loader) > 0:
    im_batch, lab_batch = next(iter(train_loader)) # get a batch
    print('Batch of images shape:', im_batch.shape) # B, C, H, W - C should be 4
    print('Batch of labels shape:', lab_batch.shape) # B, 1
    # Plot the first channel of the first image in the batch
    if im_batch.shape[0] > 0 and im_batch.shape[1] > 0:
         plt.imshow(im_batch[0,0,:,:].cpu().numpy()) # Displaying the first channel (e.g., cortical thickness)
         plt.title(f"Example from DataLoader: Channel 0, Label: {lab_batch[0].item():.2f}")
         plt.show()
    else:
        print("Batch is empty or has no channels to plot.")
else:
    print("Train loader is empty.")



# Add these imports at the beginning of your script if not already present
import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
# from qwen_vl_utils import process_vision_info # process_vision_info is part of older qwen_vl_utils
from PIL import Image # For converting tensors to PIL Images
import numpy as np
import re # For parsing the model's output
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt # For plotting

# It's recommended to run this in a separate cell at the beginning of your notebook
# Ensure these pip installs are executed in your environment
# !pip install transformers>=4.40.0 torch torchvision torchaudio accelerate peft bitsandbytes tiktoken sentencepiece qwen-vl-utils[decord]==0.0.8

# Load the Qwen model and processor
# This should be done after your initial setup and before the validation loop
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

qwen_model_name = "Qwen/Qwen2.5-VL-7B-Instruct"
qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    qwen_model_name,
    torch_dtype="auto", # or torch.float16 if using GPU and want to save memory
    device_map="auto", # Automatically uses GPU if available,
    trust_remote_code=True
)
qwen_processor = AutoProcessor.from_pretrained(qwen_model_name, trust_remote_code=True)

print(f"Qwen model loaded on: {qwen_model.device}")

# Assuming 'validation_loader' is already defined and loaded as per aml_coursework1_mllm.py
# e.g.:
# ds = BrainClassificationDataset(dirname,label_df)
# ...
# validation_loader = torch.utils.data.DataLoader(ds, batch_size=BATCH_SIZE_VAL_QWEN, sampler=valid_sampler)

# --- New Validation Loop using Qwen Model (Zero-Shot with 4 images per sample) ---
qwen_model.eval() # Set the model to evaluation mode

all_predictions_text_zero_shot = []
all_numerical_predictions_zero_shot = []
all_true_labels_zero_shot = []
all_pil_images_zero_shot = [] # For visualization

# Task instruction for the LLM for zero-shot multi-image
task_instruction_zero_shot_multi = (
    "You are provided with four 2D projection maps from a neonatal brain MRI scan. "
    "These represent, in order: Image 1: cortical thickness, Image 2: cortical curvature, "
    "Image 3: cortical myelination, and Image 4: sulcal depth. "
    "Based on these four images, predict the gestational age at scan in weeks. "
    "Provide only the numerical value of the predicted age in weeks, for example, '38.5'."
)

# print("\nStarting Zero-Shot Prediction with Qwen Model (4 images per sample) on the validation set...")

# # Fallback for vision_config if needed for image_grid_thw
# try:
#     vision_config_zero_shot = qwen_model.config.vision_config
# except AttributeError:
#     print("Warning: Could not directly access vision_config from qwen_model. Ensure model has this attribute if image_grid_thw needs manual creation.")
#     vision_config_zero_shot = None


# with torch.no_grad():
#     for batch_idx, (images_batch_cpu, labels_batch_cpu) in enumerate(tqdm(validation_loader, desc="Zero-Shot Validation")):
#         for i in range(images_batch_cpu.size(0)):
#             image_tensor_sample = images_batch_cpu[i].to(device)
#             true_label = labels_batch_cpu[i].item()

#             if image_tensor_sample.shape[0] != 4:
#                 print(f"Warning: Sample {i} in batch {batch_idx} has {image_tensor_sample.shape[0]} channels, expected 4. Skipping.")
#                 continue

#             current_sample_pil_images = []
#             for channel_idx in range(4):
#                 single_channel_tensor = image_tensor_sample[channel_idx, :, :]
#                 rgb_channel_tensor = single_channel_tensor.unsqueeze(0).repeat(3, 1, 1)
#                 pil_image_channel = Image.fromarray(
#                     (rgb_channel_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
#                 )
#                 current_sample_pil_images.append(pil_image_channel)
#             all_pil_images_zero_shot.append(current_sample_pil_images)

#             user_content = []
#             for _ in range(len(current_sample_pil_images)):
#                  user_content.append({"type": "image"})
#             user_content.append({"type": "text", "text": task_instruction_zero_shot_multi})
#             messages = [{"role": "user", "content": user_content}]
#             text_prompt_for_processor = qwen_processor.apply_chat_template(
#                 messages, tokenize=False, add_generation_prompt=True
#             )
#             inputs = qwen_processor(
#                 text=[text_prompt_for_processor],
#                 images=current_sample_pil_images,
#                 padding=True,
#                 return_tensors="pt",
#             ).to(qwen_model.device)

#             # Initialize generate_kwargs; image_grid_thw will be added conditionally
#             generate_kwargs = {"max_new_tokens": 50}

#             # Check if processor already included image_grid_thw
#             if inputs.get("image_grid_thw") is None:
#                 if vision_config_zero_shot and hasattr(vision_config_zero_shot, 'image_size') and hasattr(vision_config_zero_shot, 'patch_size'):
#                     image_size = vision_config_zero_shot.image_size
#                     patch_size = vision_config_zero_shot.patch_size
#                     if isinstance(image_size, (list, tuple)):
#                         h_patches = image_size[0] // patch_size
#                         w_patches = image_size[1] // patch_size
#                     else:
#                         h_patches = w_patches = image_size // patch_size
#                     single_image_grid = [1, h_patches, w_patches]
#                     computed_image_grid_thw = [single_image_grid for _ in current_sample_pil_images]
#                     generate_kwargs["image_grid_thw"] = computed_image_grid_thw # Add to kwargs
#                     if batch_idx == 0 and i == 0:
#                         print(f"Zero-shot: Computed and adding image_grid_thw to generate_kwargs: {computed_image_grid_thw[0]}")
#                 else:
#                     if batch_idx == 0 and i == 0:
#                         print(f"Zero-shot Warning: Cannot compute image_grid_thw (no vision_config/details) and not in processor output for B{batch_idx}, I{i}.")
#             elif batch_idx == 0 and i == 0 :
#               print(f"Zero-shot: Using image_grid_thw from processor output (in inputs). Example: {inputs.get('image_grid_thw')[0] if inputs.get('image_grid_thw') is not None and inputs.get('image_grid_thw').nelement() > 0 else 'Empty or not found'}")
#             # The `inputs` dictionary is spread via **inputs.
#             # `generate_kwargs` is spread via **generate_kwargs.
#             # If 'image_grid_thw' is in `inputs` AND in `generate_kwargs`, TypeError occurs.
#             # The logic above ensures 'image_grid_thw' is added to `generate_kwargs` ONLY IF it's not in `inputs`.
#             # Thus, a direct call should be safe.
#             generated_ids = qwen_model.generate(**inputs, **generate_kwargs)

#             generated_ids_trimmed = [
#                 out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
#             ]
#             output_text_list = qwen_processor.batch_decode(
#                 generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
#             )

#             predicted_text = output_text_list[0] if output_text_list else ""
#             all_predictions_text_zero_shot.append(predicted_text)
#             all_true_labels_zero_shot.append(true_label)

#             numerical_match = re.findall(r"[-+]?\d*\.\d+|\d+", predicted_text)
#             if numerical_match:
#                 try:
#                     numerical_pred = float(numerical_match[0])
#                     all_numerical_predictions_zero_shot.append(numerical_pred)
#                     if batch_idx < 1 and i < 2:
#                         print(f"  Zero-Shot Sample (B{batch_idx},I{i}): True={true_label:.2f}, Pred Text='{predicted_text}', Parsed={numerical_pred:.2f}")
#                 except ValueError:
#                     all_numerical_predictions_zero_shot.append(np.nan)
#                     if batch_idx < 1 and i < 2:
#                         print(f"  Zero-Shot Sample (B{batch_idx},I{i}): True={true_label:.2f}, Pred Text='{predicted_text}', Parse FAILED.")
#             else:
#                 all_numerical_predictions_zero_shot.append(np.nan)
#                 if batch_idx < 1 and i < 2:
#                     print(f"  Zero-Shot Sample (B{batch_idx},I{i}): True={true_label:.2f}, Pred Text='{predicted_text}', No number found.")

# # Calculate MSE for Zero-Shot
# valid_indices_zero_shot = [
#     idx for idx, pred in enumerate(all_numerical_predictions_zero_shot) if not (pred is None or np.isnan(pred))
# ]
# filtered_numerical_predictions_zero_shot = [all_numerical_predictions_zero_shot[i] for i in valid_indices_zero_shot]
# filtered_true_labels_zero_shot = [all_true_labels_zero_shot[i] for i in valid_indices_zero_shot]

# if filtered_numerical_predictions_zero_shot and filtered_true_labels_zero_shot:
#     mse_zero_shot = mean_squared_error(filtered_true_labels_zero_shot, filtered_numerical_predictions_zero_shot)
#     print(f"\nMean Squared Error (MSE) on the validation set (Zero-Shot, Multi-Image): {mse_zero_shot:.4f}")
#     print(f"Number of successfully parsed predictions for Zero-Shot MSE: {len(filtered_numerical_predictions_zero_shot)}/{len(all_true_labels_zero_shot)}")

#     # Display a few sample images and predictions for zero-shot
#     print("\nSample Zero-Shot Predictions (Text & Images):")
#     channel_names = ["Cortical Thickness", "Cortical Curvature", "Myelination", "Sulcal Depth"]
#     num_samples_to_show_zero_shot = min(3, len(all_predictions_text_zero_shot))
#     for k in range(num_samples_to_show_zero_shot):
#         print(f"  Sample {k+1}: True={all_true_labels_zero_shot[k]:.2f}, Predicted Text='{all_predictions_text_zero_shot[k]}'")
#         if k < len(all_pil_images_zero_shot) and all_pil_images_zero_shot[k]:
#             fig, axes = plt.subplots(1, 4, figsize=(15, 4))
#             fig.suptitle(f"Zero-Shot Sample #{k+1} - Input Images (True Age: {all_true_labels_zero_shot[k]:.2f})", fontsize=14)
#             for channel_i, pil_img in enumerate(all_pil_images_zero_shot[k]):
#                 axes[channel_i].imshow(pil_img)
#                 axes[channel_i].set_title(channel_names[channel_i])
#                 axes[channel_i].axis('off')
#             plt.tight_layout(rect=[0, 0.03, 1, 0.95])
#             plt.show()
# else:
#     print("\nCould not calculate MSE for Zero-Shot: No valid numerical predictions were parsed.")

total_params = sum(p.numel() for p in qwen_model.parameters())
trainable_params = sum(p.numel() for p in qwen_model.parameters() if p.requires_grad) # Will be 0 if not fine-tuning
non_trainable_params = total_params - trainable_params

print(f"Total params (Qwen base model): {total_params:,}")
print(f"Trainable params (Qwen base model): {trainable_params:,}") # Expected to be 0 or all if no PEFT
print(f"Non-trainable params (Qwen base model): {non_trainable_params:,}")


"""training"""

from tqdm import tqdm # 用于显示进度条
from transformers import BitsAndBytesConfig # For quantization if used

# PEFT/LoRA 相关导入
from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig

# for idx, m in enumerate(qwen_model.named_modules()): # Already done when loading qwen_model
#   print(idx, '->', m)

# Re-define model and processor loading here for training, possibly with quantization
# This ensures the training section is self-contained for model setup.
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
qwen_model_name = "Qwen/Qwen2.5-VL-7B-Instruct"

# Optional: Quantization (example, adjust as needed)
# bnb_config = BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_quant_type="nf4",
#     bnb_4bit_compute_dtype=torch.bfloat16, # or torch.float16
#     bnb_4bit_use_double_quant=True,
# )

qwen_model_for_training = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    qwen_model_name,
    torch_dtype="auto",
    device_map="auto",
    # quantization_config=bnb_config, # Uncomment if using quantization
    trust_remote_code=True
)
# qwen_processor is already loaded, can be reused.
# If reloading processor:
# qwen_processor = AutoProcessor.from_pretrained(qwen_model_name, trust_remote_code=True)


# Define LoRA Config (ensure target_modules are appropriate for Qwen2.5-VL)
# Common modules for Qwen2-VL might include specific names in 'transformer.h.[layer_idx].attn.c_attn', '...mlp.w1/w2/c_proj'
# The list below is a general example, verify against your model structure.
# You can print model.named_modules() to find suitable layers.
lora_target_modules = [
    # Vision Transformer parts (if fine-tuning, often named differently, e.g. within 'visual')
    # For Qwen2.5-VL, visual encoder modules might be like:
    # 'visual.transformer.resblocks.*.attn.qkv', 'visual.transformer.resblocks.*.attn.proj'
    # 'visual.transformer.resblocks.*.mlp.fc1', 'visual.transformer.resblocks.*.mlp.fc2'
    "visual.blocks.0.attn.qkv", # Example, verify actual names
    "visual.blocks.0.attn.proj",# Example

    # Language Model parts (more common for LoRA)
    # For Qwen2 (similar to Llama-like architectures):
    "model.layers.0.self_attn.q_proj", "model.layers.0.self_attn.k_proj",
    "model.layers.0.self_attn.v_proj", "model.layers.0.self_attn.o_proj",
    "model.layers.0.mlp.gate_proj", "model.layers.0.mlp.up_proj", "model.layers.0.mlp.down_proj",
    # You would typically list these for ALL layers you want to adapt, or use regex/keywords if supported by PEFT
    # For simplicity, often people target all 'q_proj', 'v_proj' etc.
    # Let's use a more general targeting strategy for common linear layers in attention and MLP
    # This requires inspecting the model structure of Qwen/Qwen2.5-VL-7B-Instruct
]

# A more robust way to get target_modules for Qwen2.5-VL (example):
# Inspecting qwen_model_for_training.named_modules() reveals names like:
# visual.transformer.resblocks.0.attn.qkv.weight
# model.layers.0.self_attn.q_proj.weight
# model.layers.0.mlp.gate_proj.weight
# So the module names are like 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'
# and for vision 'qkv', 'proj', 'fc1', 'fc2' (or similar names in its MLP)

# Updated lora_target_modules for Qwen2.5-VL-7B-Instruct (more comprehensive)
lora_target_modules_qwen2_5 = [
    # For the Vision Transformer (visual.blocks)
    "qkv",        # As in visual.blocks.*.attn.qkv
    "proj",       # As in visual.blocks.*.attn.proj

    # # For the Language Model (model.layers)
    # "q_proj",     # As in model.layers.*.self_attn.q_proj
    # "k_proj",     # As in model.layers.*.self_attn.k_proj
    # "v_proj",     # As in model.layers.*.self_attn.v_proj
    # "o_proj",     # As in model.layers.*.self_attn.o_proj


    # # For MLP layers in both Vision and Language parts (common names)
    # "gate_proj",  # As in visual.blocks.*.mlp.gate_proj and model.layers.*.mlp.gate_proj
    # "up_proj",    # As in visual.blocks.*.mlp.up_proj and model.layers.*.mlp.up_proj
    # "down_proj",  # As in visual.blocks.*.mlp.down_proj and model.layers.*.mlp.down_proj
]



lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=lora_target_modules_qwen2_5, # Use the dynamically identified or fallback list
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    # modules_to_save = ["lm_head", "visual.attn_pool"] # Example: if you want to also train output layer or vision pooler
)

# Apply LoRA to the model intended for training
# qwen_model_for_training.enable_input_require_grads() # May be needed for some PEFT versions/models
peft_model = get_peft_model(qwen_model_for_training, lora_config)
peft_model.print_trainable_parameters()

print(f"PEFT Qwen model for training loaded on: {peft_model.device}")

# Optimizer
optimizer = optim.AdamW(peft_model.parameters(), lr=3e-5) # Common LR for LoRA fine-tuning
num_epochs = 50 # Adjust as needed, 3-10 is common for LoRA

# Ensure pad token is set for tokenizer if not already
if qwen_processor.tokenizer.pad_token_id is None:
    qwen_processor.tokenizer.pad_token_id = qwen_processor.tokenizer.eos_token_id
    peft_model.config.pad_token_id = qwen_processor.tokenizer.pad_token_id # Also update model config
    print(f"Set tokenizer pad_token_id and model config pad_token_id to eos_token_id: {qwen_processor.tokenizer.eos_token_id}")


# --- create_training_data_for_batch function ---
# This function, as provided in your script, already correctly handles creating 4 PIL images
# per sample and structuring the prompt for multi-image input.
# The prompt text `prompt_text_question` inside it is also appropriate.
# No changes needed to this function based on your prompt, assuming it's the one from your file.
def create_training_data_for_batch(image_tensor_batch, numerical_labels_batch, processor):
    batch_full_dialogue_strings = []
    batch_prompt_only_strings = []
    batch_all_pil_images_flat = [] # Flat list of all PIL images for the batch

    # This prompt is suitable for the 4-image input task
    prompt_text_question = (
        "You are provided with four 2D projection maps from a neonatal brain MRI scan. "
        "These represent, in order: Image 1: cortical thickness, Image 2: cortical curvature, "
        "Image 3: cortical myelination, and Image 4: sulcal depth. "
        "Based on these four images, predict the gestational age at scan in days. "
        "Provide only the numerical value, for example, '269.5'."
    )

    for i in range(image_tensor_batch.size(0)): # Iterate through samples in the batch
        image_tensor = image_tensor_batch[i].cpu() # Move to CPU for numpy conversion. Shape (4, H, W)
        numerical_label = numerical_labels_batch[i].item()
        target_text_label = f"{numerical_label:.1f}" # Format label as text

        current_sample_pil_images = []
        if image_tensor.shape[0] != 4: # Ensure 4 channels
            print(f"Warning: Training sample expects 4 channels, got {image_tensor.shape[0]}. Skipping.")
            continue # Or handle error appropriately

        for channel_idx in range(4): # Process each channel as a separate image
            single_channel_tensor = image_tensor[channel_idx, :, :] # H, W
            rgb_channel_tensor = single_channel_tensor.unsqueeze(0).repeat(3, 1, 1) # 3, H, W
            pil_image_channel = Image.fromarray(
                (rgb_channel_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8) # .cpu() already done
            )
            current_sample_pil_images.append(pil_image_channel)

        batch_all_pil_images_flat.extend(current_sample_pil_images) # Add the 4 images to the flat list

        # Define messages for chat template
        system_message = {"role": "system", "content": "You are an expert radiologist. Your task is to predict gestational age."}

        user_message_content_list = []
        for _ in range(len(current_sample_pil_images)): # Add 4 image placeholders
            user_message_content_list.append({"type": "image"})
        user_message_content_list.append({"type": "text", "text": prompt_text_question})

        user_message = {"role": "user", "content": user_message_content_list}
        assistant_message = {"role": "assistant", "content": target_text_label} # Model learns to output this

        # Full dialogue for model input_ids (includes answer)
        full_dialogue_messages = [system_message, user_message, assistant_message]
        full_dialogue_str = processor.apply_chat_template(full_dialogue_messages, tokenize=False, add_generation_prompt=False)
        batch_full_dialogue_strings.append(full_dialogue_str)

        # Prompt-only for calculating mask length (excludes answer, includes generation prompt)
        prompt_only_messages = [system_message, user_message]
        prompt_only_str = processor.apply_chat_template(prompt_only_messages, tokenize=False, add_generation_prompt=True)
        batch_prompt_only_strings.append(prompt_only_str)

    return batch_full_dialogue_strings, batch_prompt_only_strings, batch_all_pil_images_flat


# Training loop
peft_model.train()
print("\nStarting PEFT Fine-tuning with Qwen Model (Multi-Image Input)...")

vision_config_train = peft_model.config.vision_config # Access through peft_model

for epoch in range(num_epochs):
    epoch_loss = 0
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")
    for batch_idx, (images_batch, labels_batch) in enumerate(progress_bar):
        optimizer.zero_grad()

        # Create training data (texts and flat list of PIL images)
        # images_batch is (Batch_size, 4, H, W)
        batch_full_dialogues, batch_prompts_only, batch_pil_images_flat = \
            create_training_data_for_batch(images_batch, labels_batch, qwen_processor)

        if not batch_full_dialogues: # Skip if a batch had issues (e.g. wrong channel count for all samples)
            print(f"Skipping empty batch {batch_idx} in epoch {epoch+1}")
            continue

        # Tokenize dialogues. `images` argument takes the flat list of all PILs for the batch.
        # The processor should correctly map images to their respective dialogues.
        tokenized_full_inputs = qwen_processor(
            text=batch_full_dialogues,
            images=batch_pil_images_flat, # Flat list of B*4 images
            padding="longest", # Pad to the longest sequence in the batch
            truncation=True,
            max_length=1024, # Adjust max_length as needed
            return_tensors="pt",
            return_attention_mask=True
        ).to(device)

        input_ids = tokenized_full_inputs.input_ids
        attention_mask = tokenized_full_inputs.attention_mask
        pixel_values_for_model = tokenized_full_inputs.get("pixel_values")

        if pixel_values_for_model is None:
            print(f"Warning: pixel_values are None for training batch {batch_idx}. Skipping batch.")
            continue
        pixel_values_for_model = pixel_values_for_model.to(peft_model.dtype)


        # --- Handle image_grid_thw ---
        image_grid_thw_input = tokenized_full_inputs.get("image_grid_thw")
        if image_grid_thw_input is None: # If processor didn't create it
            if vision_config_train and hasattr(vision_config_train, 'image_size') and hasattr(vision_config_train, 'patch_size'):
                image_size = vision_config_train.image_size
                patch_size = vision_config_train.patch_size
                if isinstance(image_size, (list, tuple)):
                    h_patches, w_patches = (image_size[0] // patch_size, image_size[1] // patch_size)
                else:
                    h_patches = w_patches = image_size // patch_size

                single_image_grid = [1, h_patches, w_patches] # t, h, w patches
                # Create a list of grids, one for each image in batch_pil_images_flat
                image_grid_thw_input = [single_image_grid for _ in range(len(batch_pil_images_flat))]
                if batch_idx == 0 and epoch == 0:
                    print(f"Training: Computed default image_grid_thw for {len(batch_pil_images_flat)} images.")
            else:
                print(f"Error: Cannot compute default image_grid_thw for training batch {batch_idx}. Skipping.")
                continue

        # Create labels: mask prompt tokens, keep only answer tokens
        labels = input_ids.clone()
        tokenized_prompts = qwen_processor(
            text=batch_prompts_only, # Prompts without answers
            images=batch_pil_images_flat, # Images are needed for prompt tokenization too if placeholders affect length
            padding="longest",
            truncation=True,
            max_length=1024,
            return_tensors="pt",
            return_attention_mask=True, # For getting prompt length
            add_special_tokens=True # Ensure consistent tokenization with full dialogue
        ).to(device)

        for i in range(input_ids.size(0)): # Iterate through samples in the batch
            # Length of the prompt part (system + user + image placeholders + assistant prompt)
            prompt_length = tokenized_prompts.attention_mask[i].sum().item()
            labels[i, :prompt_length] = -100 # Mask prompt tokens

        labels[input_ids == qwen_processor.tokenizer.pad_token_id] = -100 # Mask padding tokens
        labels = labels.to(device)

        # Forward pass
        try:
            outputs = peft_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                pixel_values=pixel_values_for_model,
                image_grid_thw=image_grid_thw_input,
                labels=labels
            )
            loss = outputs.loss
        except Exception as e:
            print(f"Error during model forward pass for training batch {batch_idx}: {e}")
            print(f"  input_ids shape: {input_ids.shape if input_ids is not None else 'None'}")
            print(f"  pixel_values shape: {pixel_values_for_model.shape if pixel_values_for_model is not None else 'None'}")
            # print(f"  image_grid_thw: {image_grid_thw_input[0] if image_grid_thw_input else 'None'} (example of first grid)")
            print(f"  labels shape: {labels.shape if labels is not None else 'None'}")
            continue


        if loss is None:
            print(f"Error: Loss is None for training batch {batch_idx}. Check inputs/labels.")
            continue

        loss.backward()
        torch.nn.utils.clip_grad_norm_(peft_model.parameters(), max_norm=1.0)
        optimizer.step()

        epoch_loss += loss.item()
        progress_bar.set_postfix({'loss': loss.item()})

    avg_epoch_loss = epoch_loss / len(train_loader) if len(train_loader) > 0 else float('inf')
    print(f"Epoch {epoch+1}/{num_epochs} - Average Training Loss: {avg_epoch_loss:.4f}")

    # Optional: Add validation step within the training loop using a multi-image validation function
    # validate_finetuned_model_multi_image(peft_model, qwen_processor, validation_loader, device, task_instruction_for_validation_multi, current_epoch=epoch)

print("Fine-tuning finished.")

# --- Save LoRA adapter ---
lora_adapter_path = "/content/drive/MyDrive/pma_prediction_mllm/qwen_2_5_7b_vl_lora_adapter_gestational_age_multi_image_test"
if not os.path.exists(os.path.dirname(lora_adapter_path)):
    os.makedirs(os.path.dirname(lora_adapter_path))
peft_model.save_pretrained(lora_adapter_path)
print(f"LoRA adapter saved to {lora_adapter_path}")


# --- `validate_finetuned_model_multi_image` function ---
# This function from your script is already structured for multi-image input.
# We'll ensure its prompt and image_grid_thw logic are robust.

def validate_finetuned_model_multi_image(
    model_to_validate,
    processor,
    val_loader,
    device,
    task_instruction, # Pass the specific multi-image instruction string
    max_new_tokens_generation=50,
    display_images_and_details=False,
    epoch_num=-1 # For logging
):
    model_to_validate.eval()
    all_parsed_numerical_predictions = []
    all_raw_text_predictions = []
    all_true_labels = []
    all_image_sets_for_display = []

    # Determine vision_config for image_grid_thw fallback
    current_vision_config = None
    try:
        if hasattr(model_to_validate, 'config') and hasattr(model_to_validate.config, 'vision_config'):
            current_vision_config = model_to_validate.config.vision_config
        elif hasattr(model_to_validate, 'base_model') and hasattr(model_to_validate.base_model, 'model') and \
             hasattr(model_to_validate.base_model.model, 'config') and hasattr(model_to_validate.base_model.model.config, 'vision_config'):
            current_vision_config = model_to_validate.base_model.model.config.vision_config
        # Add more checks if PEFT model structure varies
        else: # Fallback for PeftModel
            if hasattr(model_to_validate, 'get_base_model') and hasattr(model_to_validate.get_base_model().config, 'vision_config'):
                 current_vision_config = model_to_validate.get_base_model().config.vision_config
            else:
                 print("validate_finetuned_model_multi_image: Could not determine vision_config reliably.")
    except Exception as e:
        print(f"validate_finetuned_model_multi_image: Error accessing vision_config: {e}")


    desc = f"Validation Epoch {epoch_num+1}" if epoch_num != -1 else "Final Validation (Multi-Image)"
    print(f"\nStarting {desc} (display_images={display_images_and_details})...")

    with torch.no_grad():
        progress_bar_val = tqdm(val_loader, desc=desc)
        for batch_idx, (images_batch_cpu, labels_batch_cpu) in enumerate(progress_bar_val):
            for i in range(images_batch_cpu.size(0)): # Process each sample in batch
                image_tensor_sample = images_batch_cpu[i].to(device) # (4, H, W)
                true_label = labels_batch_cpu[i].item()
                all_true_labels.append(true_label)

                if image_tensor_sample.shape[0] != 4:
                    print(f"Warning: Val sample expects 4 channels, got {image_tensor_sample.shape[0]}. Skipping.")
                    all_raw_text_predictions.append("Error: Incorrect channel count")
                    all_parsed_numerical_predictions.append(np.nan)
                    if display_images_and_details: all_image_sets_for_display.append([])
                    continue

                current_sample_pil_images = []
                for channel_idx in range(4):
                    single_channel_tensor = image_tensor_sample[channel_idx, :, :]
                    rgb_channel_tensor = single_channel_tensor.unsqueeze(0).repeat(3, 1, 1)
                    pil_image_channel = Image.fromarray(
                        (rgb_channel_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
                    )
                    current_sample_pil_images.append(pil_image_channel)

                if display_images_and_details:
                    all_image_sets_for_display.append(current_sample_pil_images)

                # Construct prompt with 4 image placeholders
                user_content_val = []
                for _ in range(len(current_sample_pil_images)):
                    user_content_val.append({"type": "image"})
                user_content_val.append({"type": "text", "text": task_instruction}) # Use the passed multi-image instruction

                messages_val = [
                     {"role": "system", "content": "You are an expert radiologist interpreting neonatal brain MRI scans."},
                     {"role": "user", "content": user_content_val}
                ]
                text_prompt_val = processor.apply_chat_template(
                    messages_val, tokenize=False, add_generation_prompt=True
                )
                inputs_val = processor(
                    text=[text_prompt_val],
                    images=current_sample_pil_images, # List of 4 PILs
                    padding=True,
                    return_tensors="pt",
                    return_attention_mask=True # Ensure attention_mask is returned
                ).to(device)

                # Initialize generate_kwargs
                generate_kwargs_val = {
                    "max_new_tokens": max_new_tokens_generation,
                    "do_sample": False
                }
                if processor.tokenizer.pad_token_id is not None:
                    generate_kwargs_val["pad_token_id"] = processor.tokenizer.pad_token_id
                else:
                    generate_kwargs_val["pad_token_id"] = processor.tokenizer.eos_token_id

                # Conditionally add image_grid_thw to generate_kwargs_val
                if inputs_val.get("image_grid_thw") is None: # Check if NOT in processor output
                    if current_vision_config: # And if we have config to compute it
                        if hasattr(current_vision_config, 'image_size') and hasattr(current_vision_config, 'patch_size'):
                            image_size = current_vision_config.image_size
                            patch_size = current_vision_config.patch_size
                            if isinstance(image_size, (list,tuple)):
                                h_p, w_p = image_size[0]//patch_size, image_size[1]//patch_size
                            else:
                                h_p = w_p = image_size // patch_size
                            single_grid = [1, h_p, w_p]
                            computed_igthw = [single_grid for _ in current_sample_pil_images]
                            generate_kwargs_val["image_grid_thw"] = computed_igthw
                            if batch_idx == 0 and i == 0:
                                print(f"Val (multi-image): Computed and adding image_grid_thw to generate_kwargs_val.")
                        else:
                            if batch_idx == 0 and i == 0:
                                print(f"Val (multi-image) Warning: Cannot compute image_grid_thw for B{batch_idx},I{i}")
                    else:
                        if batch_idx == 0 and i == 0:
                            print(f"Val (multi-image) Warning: image_grid_thw not in processor output and no vision_config B{batch_idx},I{i}")
                elif batch_idx == 0 and i == 0:
                     print(f"Val (multi-image): Using image_grid_thw from processor output (in inputs_val).")

                try:
                    # **inputs_val will spread all keys from processor output (input_ids, pixel_values, attention_mask, and potentially image_grid_thw)
                    # **generate_kwargs_val will spread other generation params.
                    # The logic above ensures image_grid_thw is not in both.
                    generated_ids = model_to_validate.generate(**inputs_val, **generate_kwargs_val)
                except Exception as e_generate:
                    print(f"Error during model.generate for val item B{batch_idx}, I{i}: {e_generate}")
                    all_raw_text_predictions.append(f"Error: Gen failed - {e_generate}"); all_parsed_numerical_predictions.append(np.nan)

                    continue

                generated_ids_trimmed = generated_ids[:, inputs_val.shape[1]:]
                output_text_list = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)
                predicted_text_full = output_text_list[0] if output_text_list else ""
                all_raw_text_predictions.append(predicted_text_full)

                numerical_match = re.findall(r"[-+]?\d*\.\d+|\d+", predicted_text_full.split('\n')[0]) # Try first line
                if not numerical_match: numerical_match = re.findall(r"[-+]?\d*\.\d+|\d+", predicted_text_full) # Try whole text
                if numerical_match:
                    try: all_parsed_numerical_predictions.append(float(numerical_match[0]))
                    except ValueError: all_parsed_numerical_predictions.append(np.nan)
                else: all_parsed_numerical_predictions.append(np.nan)

                if batch_idx == 0 and i < 2 : # Print a few samples
                    parsed_val_str = f"{all_parsed_numerical_predictions[-1]:.1f}" if not np.isnan(all_parsed_numerical_predictions[-1]) else "N/A"
                    print(f"  Val Sample (B{batch_idx},I{i}): True={true_label:.1f}, Parsed={parsed_val_str}, Output='{predicted_text_full[:100]}...'")

    # MSE Calculation (identical logic to zero-shot, just different variable names)
    valid_preds_mse = [p for p in all_parsed_numerical_predictions if not np.isnan(p)]
    corr_true_labels_mse = [all_true_labels[i] for i, p in enumerate(all_parsed_numerical_predictions) if not np.isnan(p)]
    calculated_mse = float('inf')
    if valid_preds_mse and corr_true_labels_mse:
        calculated_mse = mean_squared_error(corr_true_labels_mse, valid_preds_mse)
        print(f"{desc} Mean Squared Error (MSE): {calculated_mse:.4f}")
        print(f"Number of successfully parsed predictions for MSE: {len(valid_preds_mse)}/{len(all_true_labels)}")
    else: print(f"{desc}: Could not calculate MSE: No valid numerical predictions.")

    if display_images_and_details:
        print(f"\n--- Detailed {desc} Results (with Images, Predictions) ---")
        num_samples_to_show = min(len(all_true_labels), 51)
        channel_names_detail = ["Cortical Thickness", "Cortical Curvature", "Myelination", "Sulcal Depth"]
        for idx in range(num_samples_to_show):
            true_str = f"{all_true_labels[idx]:.1f}" if idx < len(all_true_labels) else "N/A"
            pred_text = all_raw_text_predictions[idx] if idx < len(all_raw_text_predictions) else "Err"
            parsed_str = "N/A"
            if idx < len(all_parsed_numerical_predictions) and not np.isnan(all_parsed_numerical_predictions[idx]):
                parsed_str = f"{all_parsed_numerical_predictions[idx]:.1f}"
            print(f"\nSample #{idx+1}: True={true_str}, Parsed={parsed_str}, FullOutput='{pred_text}'")
            if idx < len(all_image_sets_for_display) and all_image_sets_for_display[idx]:
                fig, axes = plt.subplots(1, 4, figsize=(20, 5))
                fig.suptitle(f"Sample #{idx+1} - Inputs (True: {true_str})", fontsize=16)
                for ch_i, pil_img in enumerate(all_image_sets_for_display[idx]):
                    axes[ch_i].imshow(pil_img); axes[ch_i].set_title(channel_names_detail[ch_i]); axes[ch_i].axis('off')
                plt.tight_layout(rect=[0,0.03,1,0.95]); plt.show()
        print(f"\n--- End of Detailed {desc} Results ---")
    model_to_validate.train()
    return calculated_mse

# Example call to this validation function (after training or with a loaded model)
# Define the multi-image task instruction for validation numerical output:
task_instruction_validate_numerical_multi = (
    "You are provided with four 2D projection maps from a neonatal brain MRI scan. "
    "These represent, in order: Image 1: cortical thickness, Image 2: cortical curvature, "
    "Image 3: cortical myelination, and Image 4: sulcal depth. "
    "Based on these four images, predict the gestational age at scan in days. "
    "Provide only the numerical value, for example, '269.5'."
)
# If peft_model is trained, you can validate it:
# validate_finetuned_model_multi_image(peft_model, qwen_processor, validation_loader, device,
#                                     task_instruction_validate_numerical_multi, display_images_and_details=True)

"""Loading the fine-tuned model"""

from tqdm import tqdm
from transformers import BitsAndBytesConfig # For quantization if used
from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig
import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
# from qwen_vl_utils import process_vision_info # process_vision_info is part of older qwen_vl_utils
from PIL import Image # For converting tensors to PIL Images
import numpy as np
import re # For parsing the model's output
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt # For plotting

lora_adapter_path = "/content/drive/MyDrive/pma_prediction_mllm/qwen_2_5_7b_vl_lora_adapter_gestational_age_multi_image_test"
try:
    # 1. Load base model config from saved LoRA adapter
    config_loaded = PeftConfig.from_pretrained(lora_adapter_path)
    base_model_name_or_path = config_loaded.base_model_name_or_path

    # 2. Load base model
    base_model_loaded = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        base_model_name_or_path,
        torch_dtype="auto",
        device_map="auto",
        trust_remote_code=True
    )
    # Processor can be reloaded or reused if compatible
    processor_loaded = AutoProcessor.from_pretrained(base_model_name_or_path, trust_remote_code=True)
    if processor_loaded.tokenizer.pad_token_id is None: # Ensure pad token for loaded processor
        processor_loaded.tokenizer.pad_token_id = processor_loaded.tokenizer.eos_token_id
        base_model_loaded.config.pad_token_id = processor_loaded.tokenizer.pad_token_id


    # 3. Load LoRA adapter into the base model
    loaded_peft_model = PeftModel.from_pretrained(base_model_loaded, lora_adapter_path)
    loaded_peft_model.eval() # Set to evaluation mode
    print(f"LoRA adapter from {lora_adapter_path} loaded successfully onto base model {base_model_name_or_path}.")
    print(f"Loaded PEFT model is on: {loaded_peft_model.device}")

    # Example: Validate the loaded PEFT model
    # validate_finetuned_model_multi_image(loaded_peft_model, processor_loaded, validation_loader, device,
    #                                      task_instruction_validate_numerical_multi, display_images_and_details=True)

except Exception as e:
    print(f"Error loading PEFT model from {lora_adapter_path}: {e}")
    print("Skipping operations with loaded_peft_model.")
    loaded_peft_model = None # Ensure it's None if loading failed
    processor_loaded = qwen_processor # Fallback to original processor if new one not loaded

"""Uploading the model to Huggingface"""

from huggingface_hub import notebook_login

notebook_login()

import torch
from peft import PeftModel, PeftConfig
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor

# --- 1. Define Paths and Model Name ---
# The path to your saved LoRA adapter on Google Drive
lora_adapter_path = "/content/drive/MyDrive/pma_prediction_mllm/qwen_2_5_7b_vl_lora_adapter_gestational_age_multi_image_test"

# The name for your new model repository on the Hugging Face Hub
# Replace "your-username" with your actual Hugging Face username
hf_model_repo = "Jimcui0508/qwen2.5-7b-vl-gestational-age-predictor"

# --- 2. Load the Base Model and Processor ---
# Load the configuration from the saved adapter to get the base model name
config = PeftConfig.from_pretrained(lora_adapter_path)
base_model_name = config.base_model_name_or_path

print(f"Loading base model: {base_model_name}")
base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    base_model_name,
    torch_dtype="auto",
    device_map="auto", # Use 'cpu' if you run out of GPU memory for merging
    trust_remote_code=True
)

processor = AutoProcessor.from_pretrained(base_model_name, trust_remote_code=True)

# --- 3. Load the LoRA Adapter onto the Model ---
print("Loading LoRA adapter...")
# This creates a PeftModel, which is the base model + the adapter
peft_model = PeftModel.from_pretrained(base_model, lora_adapter_path)
peft_model.eval()
print("LoRA adapter loaded successfully.")

# --- 4. Merge the Adapter with the Base Model ---
print("Merging LoRA weights into the base model...")
# This merges the adapter weights and returns a new, standalone Qwen2_5_VLForConditionalGeneration model
merged_model = peft_model.merge_and_unload()
print("Model merged successfully.")

# --- 5. Upload to Hugging Face Hub ---
print(f"Uploading merged model to {hf_model_repo}...")
# The `create_repo=True` argument will create the repository if it doesn't exist
merged_model.push_to_hub(hf_model_repo, create_repo=True)
print("Model uploaded.")

print(f"Uploading processor to {hf_model_repo}...")
# Upload the processor to the same repository
processor.push_to_hub(hf_model_repo)
print("Processor uploaded.")

print("\n✅ All done! Your model is now available on the Hugging Face Hub.")

"""1.   Infering with special prompt setting **(unit: day)**
2.   Model interpretability: results explanation


"""

import torch
import numpy as np
import re
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def validate_model_for_visualization( # Renamed to avoid conflict
    model_to_validate,
    processor,
    val_loader,
    device,
    epoch_num=-1 # For logging
):
    model_to_validate.eval()
    all_parsed_numerical_predictions = []
    all_raw_text_outputs = []
    all_true_labels = []
    all_image_sets_for_viz = [] # Store sets of 4 PIL images for each sample


    visualization_task_instruction_multi = (
        "You are provided with four 2D projection maps from a neonatal brain MRI scan. "
        "These represent, in order: Image 1: cortical thickness, Image 2: cortical curvature, "
        "Image 3: cortical myelination, and Image 4: sulcal depth. "
        "Based on these four images, please first predict the gestational age at the time of the scan (in days, e.g., '269.5').\n"
        "Then, on a new line, provide the primary reason for your age prediction.\n\n"
        "Your explanation should reference key brain developmental features visible or inferred from the maps, consistent with the predicted age. Consider features like:\n"
        "- Cortical maturity (e.g., thickness, curvature, sulcal depth)\n"
        "- Myelination progress\n"
        "- Gyrification and sulcation complexity (folding patterns)\n"
        "- Other relevant structural or signal characteristics.\n\n"
        "Please use the following format for your answer:\n"
        "Predicted Gestational Age: [Value]\n"
        "Explanation: [Your explanation]"
    )


    current_vision_config_viz = None
    try: # Get vision_config for image_grid_thw fallback
        if hasattr(model_to_validate, 'config') and hasattr(model_to_validate.config, 'vision_config'):
            current_vision_config_viz = model_to_validate.config.vision_config
        elif hasattr(model_to_validate, 'get_base_model'): # For PEFT models
                current_vision_config_viz = model_to_validate.get_base_model().config.vision_config
        else: print("Visualization Validation: Cannot determine vision_config.")
    except Exception: print("Visualization Validation: Error getting vision_config.")

    desc = f"Visualization Validation Epoch {epoch_num+1}" if epoch_num != -1 else "Final Visualization Validation"
    print(f"\nStarting {desc}...")

    with torch.no_grad():
        progress_bar_val = tqdm(val_loader, desc=desc)
        for batch_idx, (images_batch_cpu, labels_batch_cpu) in enumerate(progress_bar_val):
            for i in range(images_batch_cpu.size(0)):
                image_tensor_sample = images_batch_cpu[i].to(device) # (4, H, W)
                true_label = labels_batch_cpu[i].item()
                all_true_labels.append(true_label)

                if image_tensor_sample.shape[0] != 4: continue

                current_sample_pil_images_viz = []
                for channel_idx in range(4):
                    single_ch_t = image_tensor_sample[channel_idx, :, :]
                    rgb_ch_t = single_ch_t.unsqueeze(0).repeat(3,1,1)
                    pil_img = Image.fromarray((rgb_ch_t.permute(1,2,0).cpu().numpy()*255).astype(np.uint8))
                    current_sample_pil_images_viz.append(pil_img)
                all_image_sets_for_viz.append(current_sample_pil_images_viz)

                user_content_viz = []
                for _ in range(len(current_sample_pil_images_viz)): user_content_viz.append({"type": "image"})
                user_content_viz.append({"type": "text", "text": visualization_task_instruction_multi})

                messages_viz = [{"role": "system", "content": "You are an expert radiologist. Your task is to predict gestational age in days."}, {"role": "user", "content": user_content_viz}]
                prompt_viz = processor.apply_chat_template(messages_viz, tokenize=False, add_generation_prompt=True)
                inputs_viz = processor(text=[prompt_viz], images=current_sample_pil_images_viz, padding=True, return_tensors="pt").to(device)

                input_ids_viz = inputs_viz.input_ids
                pixel_values_viz = inputs_viz.get("pixel_values")
                if pixel_values_viz is None: continue
                pixel_values_viz = pixel_values_viz.to(model_to_validate.dtype)

                image_grid_thw_viz = inputs_viz.get("image_grid_thw")
                if image_grid_thw_viz is None and current_vision_config_viz:
                    if hasattr(current_vision_config_viz, 'image_size') and hasattr(current_vision_config_viz, 'patch_size'):
                        img_s, patch_s = current_vision_config_viz.image_size, current_vision_config_viz.patch_size
                        h_p,w_p = (img_s[0]//patch_s,img_s[1]//patch_s) if isinstance(img_s,tuple) else (img_s//patch_s,img_s//patch_s)
                        single_grid_viz = [1,h_p,w_p]
                        image_grid_thw_viz = [single_grid_viz for _ in current_sample_pil_images_viz]

                gen_kwargs_viz = {"max_new_tokens": 150, "do_sample": False} # For brief explanation
                if image_grid_thw_viz is not None and (not isinstance(image_grid_thw_viz, torch.Tensor) or image_grid_thw_viz.nelement() > 0):
                  gen_kwargs_viz["image_grid_thw"] = image_grid_thw_viz
                if processor.tokenizer.pad_token_id: gen_kwargs_viz["pad_token_id"] = processor.tokenizer.pad_token_id
                else: gen_kwargs_viz["pad_token_id"] = processor.tokenizer.eos_token_id

                try:
                    generated_ids_viz = model_to_validate.generate(input_ids=input_ids_viz, pixel_values=pixel_values_viz,
                                                                   attention_mask=inputs_viz.attention_mask, **gen_kwargs_viz)
                except Exception as e_gen_viz:
                    print(f"Viz Gen Error B{batch_idx}I{i}: {e_gen_viz}")
                    all_raw_text_outputs.append("Error: Generation failed")
                    all_parsed_numerical_predictions.append(np.nan)
                    continue

                generated_ids_trimmed_viz = generated_ids_viz[:, input_ids_viz.shape[1]:]
                output_text_full_viz = processor.batch_decode(generated_ids_trimmed_viz, skip_special_tokens=True)[0]
                all_raw_text_outputs.append(output_text_full_viz)

                num_match_viz = re.findall(r"[-+]?\d*\.\d+|\d+", output_text_full_viz.split('\n')[0])
                if not num_match_viz: num_match_viz = re.findall(r"[-+]?\d*\.\d+|\d+", output_text_full_viz)
                if num_match_viz:
                    try: all_parsed_numerical_predictions.append(float(num_match_viz[0]))
                    except ValueError: all_parsed_numerical_predictions.append(np.nan)
                else: all_parsed_numerical_predictions.append(np.nan)

    valid_predictions = [p for p in all_parsed_numerical_predictions if not np.isnan(p)]
    corresponding_true_labels = [all_true_labels[i] for i,p in enumerate(all_parsed_numerical_predictions) if not np.isnan(p)]

    metrics = {
        'mse': float('inf'), 'mae': float('inf'), 'r2': float('-inf'),
        'mse_ci_95': (0, 0), 'mae_ci_95': (0, 0), 'r2_ci_95': (0, 0)
    }

    if valid_predictions and corresponding_true_labels:
        # Calculate main metrics on the full dataset
        metrics['mse'] = mean_squared_error(corresponding_true_labels, valid_predictions)
        metrics['mae'] = mean_absolute_error(corresponding_true_labels, valid_predictions)
        metrics['r2'] = r2_score(corresponding_true_labels, valid_predictions)

        print(f"\n--- {desc} Overall Metrics ---")
        print(f"Total Samples Processed: {len(all_true_labels)}")
        print(f"Valid Samples for Metrics: {len(valid_predictions)}")
        print(f"MSE (Mean Squared Error):       {metrics['mse']:.4f}")
        print(f"MAE (Mean Absolute Error):      {metrics['mae']:.4f}")
        print(f"R2 (R-squared):                 {metrics['r2']:.4f}")
        print("--------------------------------" + "-"*len(desc))

        # =========================================================================
        # NEW: Monte Carlo (Bootstrap) Confidence Interval Calculation
        # =========================================================================
        n_iterations = 1000  # Number of bootstrap iterations
        sample_size = len(valid_predictions)

        if len(valid_predictions) >= sample_size:
            print(f"\n--- Monte Carlo 95% Confidence Intervals (N={n_iterations}, Sample Size={sample_size}) ---")
            bootstrapped_mses, bootstrapped_maes, bootstrapped_r2s = [], [], []

            valid_predictions_np = np.array(valid_predictions)
            corresponding_true_labels_np = np.array(corresponding_true_labels)
            n_total_valid_samples = len(valid_predictions_np)

            for _ in range(n_iterations):
                # Draw random indices with replacement (this is bootstrapping)
                indices = np.random.choice(n_total_valid_samples, size=sample_size, replace=True)

                y_true_sample = corresponding_true_labels_np[indices]
                y_pred_sample = valid_predictions_np[indices]

                # Calculate metrics for this small sample and store them
                bootstrapped_mses.append(mean_squared_error(y_true_sample, y_pred_sample))
                bootstrapped_maes.append(mean_absolute_error(y_true_sample, y_pred_sample))
                # Handle cases where R2 is undefined for a small constant sample
                if len(np.unique(y_true_sample)) > 1:
                    bootstrapped_r2s.append(r2_score(y_true_sample, y_pred_sample))

            # Calculate 95% CIs from the bootstrapped distributions
            if bootstrapped_mses:
                metrics['mse_ci_95'] = (np.percentile(bootstrapped_mses, 2.5), np.percentile(bootstrapped_mses, 97.5))
                print(f"MSE 95% CI:       ({metrics['mse_ci_95'][0]:.4f}, {metrics['mse_ci_95'][1]:.4f})")
            if bootstrapped_maes:
                metrics['mae_ci_95'] = (np.percentile(bootstrapped_maes, 2.5), np.percentile(bootstrapped_maes, 97.5))
                print(f"MAE 95% CI:       ({metrics['mae_ci_95'][0]:.4f}, {metrics['mae_ci_95'][1]:.4f})")
            if bootstrapped_r2s:
                metrics['r2_ci_95'] = (np.percentile(bootstrapped_r2s, 2.5), np.percentile(bootstrapped_r2s, 97.5))
                print(f"R2 95% CI:        ({metrics['r2_ci_95'][0]:.4f}, {metrics['r2_ci_95'][1]:.4f})")
            print("----------------------------------------------------------------------")

        else:
            print("\nSkipping Monte Carlo analysis: not enough valid samples (need at least 10).")
        # =========================================================================
        # END OF NEW CODE
        # =========================================================================

    else:
        print(f"\n{desc}: Could not calculate metrics (no valid predictions).")

    # Detailed Visualization
    print(f"\n--- {desc} Detailed Results (with Images and Output) ---")
    num_samples_to_show_viz = min(len(all_true_labels), 51) # Show up to 51
    channel_names_viz = ["Cortical Thickness", "Cortical Curvature", "Myelination", "Sulcal Depth"]

    for idx in range(num_samples_to_show_viz):
        print("\n" + "="*80 + f"\nSample #{idx+1}" + "\n" + "-"*80)
        true_val_str = f"{all_true_labels[idx]:.1f}"
        pred_text_str = all_raw_text_outputs[idx] if idx < len(all_raw_text_outputs) else "N/A"
        parsed_val_str = "N/A"
        if idx < len(all_parsed_numerical_predictions) and not np.isnan(all_parsed_numerical_predictions[idx]):
            parsed_val_str = f"{all_parsed_numerical_predictions[idx]:.1f}"

        print(f"True Label         : {true_val_str}")
        print(f"Parsed Prediction  : {parsed_val_str}")
        print(f"Full Model Output  :\n{pred_text_str}")
        print("-" * 80)

        if idx < len(all_image_sets_for_viz) and all_image_sets_for_viz[idx]:
            fig, axes = plt.subplots(1, 4, figsize=(20, 5))
            fig.suptitle(f"Sample #{idx+1} - Input Images (True Age: {true_val_str})", fontsize=16)
            for channel_i, pil_img_viz in enumerate(all_image_sets_for_viz[idx]):
                axes[channel_i].imshow(pil_img_viz)
                axes[channel_i].set_title(channel_names_viz[channel_i])
                axes[channel_i].axis('off')
            plt.tight_layout(rect=[0, 0.03, 1, 0.95])
            plt.show()
        else: print("(Image data not available for this sample's display)")
        print("="*80)
    print(f"\n--- End of {desc} Detailed Results (Displayed {num_samples_to_show_viz} samples) ---")

    model_to_validate.train()
    return metrics # Return the dictionary containing all metrics and CIs


# The rest of your script that calls this function can remain the same.
# If you want to use the returned CIs, you can now access them from the result.
# Example:
# validation_results = validate_model_for_visualization(...)
# print("MAE 95% Confidence Interval:", validation_results['mae_ci_95'])

if 'peft_model' in locals() and peft_model is not None and peft_model.parameters().__next__().requires_grad : # If peft_model exists and was trained
    print("\nValidating the fine-tuned PEFT model (peft_model) with visualization...")
    peft_metrics = validate_model_for_visualization(peft_model, qwen_processor, validation_loader, device)
    print("\nFinal Validation Metrics Dictionary:", peft_metrics)
elif 'loaded_peft_model' in locals() and loaded_peft_model is not None:
    print("\nValidating the loaded PEFT model (loaded_peft_model) with visualization...")
    loaded_metrics = validate_model_for_visualization(loaded_peft_model, processor_loaded if 'processor_loaded' in locals() else qwen_processor, validation_loader, 'cuda:0')
    print("\nFinal Validation Metrics Dictionary:", loaded_metrics)
else:
    print("\nNo fine-tuned or loaded PEFT model available for final visualization validation.")

"""Infering with special prompt setting **(unit: week)**"""

import torch
import numpy as np
import re
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score # <--- 增加了 MAE 和 R2

def validate_model_for_visualization( # Renamed to avoid conflict
    model_to_validate,
    processor,
    val_loader,
    device,
    epoch_num=-1 # For logging
):
    model_to_validate.eval()
    all_parsed_numerical_predictions = []
    all_raw_text_outputs = []
    all_true_labels = []
    all_image_sets_for_viz = [] # Store sets of 4 PIL images for each sample

    visualization_task_instruction_multi = (
        "You are provided with four 2D projection maps from a neonatal brain MRI scan. "
        "These represent, in order: Image 1: cortical thickness, Image 2: cortical curvature, "
        "Image 3: cortical myelination, and Image 4: sulcal depth. "
        "Based on these four images, please first predict the gestational age at the time of the scan (in weeks, e.g., '38.5').\n"
        "Then, on a new line, provide the primary reason for your age prediction.\n\n"
        "Your explanation should reference key brain developmental features visible or inferred from the maps, consistent with the predicted age. Consider features like:\n"
        "- Cortical maturity (e.g., thickness, curvature, sulcal depth)\n"
        "- Myelination progress\n"
        "- Gyrification and sulcation complexity (folding patterns)\n"
        "- Other relevant structural or signal characteristics.\n\n"
        "Please use the following format for your answer:\n"
        "Predicted Gestational Age: [Value]\n"
        "Explanation: [Your explanation]"
    )


    current_vision_config_viz = None
    try: # Get vision_config for image_grid_thw fallback
        if hasattr(model_to_validate, 'config') and hasattr(model_to_validate.config, 'vision_config'):
            current_vision_config_viz = model_to_validate.config.vision_config
        elif hasattr(model_to_validate, 'get_base_model'): # For PEFT models
                current_vision_config_viz = model_to_validate.get_base_model().config.vision_config
        else: print("Visualization Validation: Cannot determine vision_config.")
    except Exception: print("Visualization Validation: Error getting vision_config.")

    desc = f"Visualization Validation Epoch {epoch_num+1}" if epoch_num != -1 else "Final Visualization Validation"
    print(f"\nStarting {desc}...")

    with torch.no_grad():
        progress_bar_val = tqdm(val_loader, desc=desc)
        for batch_idx, (images_batch_cpu, labels_batch_cpu) in enumerate(progress_bar_val):
            for i in range(images_batch_cpu.size(0)):
                image_tensor_sample = images_batch_cpu[i].to(device) # (4, H, W)
                true_label = labels_batch_cpu[i].item()
                all_true_labels.append(true_label)

                if image_tensor_sample.shape[0] != 4: continue

                current_sample_pil_images_viz = []
                for channel_idx in range(4):
                    single_ch_t = image_tensor_sample[channel_idx, :, :]
                    rgb_ch_t = single_ch_t.unsqueeze(0).repeat(3,1,1)
                    pil_img = Image.fromarray((rgb_ch_t.permute(1,2,0).cpu().numpy()*255).astype(np.uint8))
                    current_sample_pil_images_viz.append(pil_img)
                all_image_sets_for_viz.append(current_sample_pil_images_viz)

                user_content_viz = []
                for _ in range(len(current_sample_pil_images_viz)): user_content_viz.append({"type": "image"})
                user_content_viz.append({"type": "text", "text": visualization_task_instruction_multi})

                messages_viz = [{"role": "system", "content": "You are an expert radiologist. Your task is to predict gestational age."}, {"role": "user", "content": user_content_viz}]
                prompt_viz = processor.apply_chat_template(messages_viz, tokenize=False, add_generation_prompt=True)
                inputs_viz = processor(text=[prompt_viz], images=current_sample_pil_images_viz, padding=True, return_tensors="pt").to(device)

                input_ids_viz = inputs_viz.input_ids
                pixel_values_viz = inputs_viz.get("pixel_values")
                if pixel_values_viz is None: continue
                pixel_values_viz = pixel_values_viz.to(model_to_validate.dtype)

                image_grid_thw_viz = inputs_viz.get("image_grid_thw")
                if image_grid_thw_viz is None and current_vision_config_viz:
                    if hasattr(current_vision_config_viz, 'image_size') and hasattr(current_vision_config_viz, 'patch_size'):
                        img_s, patch_s = current_vision_config_viz.image_size, current_vision_config_viz.patch_size
                        h_p,w_p = (img_s[0]//patch_s,img_s[1]//patch_s) if isinstance(img_s,tuple) else (img_s//patch_s,img_s//patch_s)
                        single_grid_viz = [1,h_p,w_p]
                        image_grid_thw_viz = [single_grid_viz for _ in current_sample_pil_images_viz]

                gen_kwargs_viz = {"max_new_tokens": 150, "do_sample": False} # For brief explanation
                if image_grid_thw_viz is not None and (not isinstance(image_grid_thw_viz, torch.Tensor) or image_grid_thw_viz.nelement() > 0):
                  gen_kwargs_viz["image_grid_thw"] = image_grid_thw_viz
                if processor.tokenizer.pad_token_id: gen_kwargs_viz["pad_token_id"] = processor.tokenizer.pad_token_id
                else: gen_kwargs_viz["pad_token_id"] = processor.tokenizer.eos_token_id

                try:
                    generated_ids_viz = model_to_validate.generate(input_ids=input_ids_viz, pixel_values=pixel_values_viz,
                                                                   attention_mask=inputs_viz.attention_mask, **gen_kwargs_viz)
                except Exception as e_gen_viz:
                    print(f"Viz Gen Error B{batch_idx}I{i}: {e_gen_viz}")
                    all_raw_text_outputs.append("Error: Generation failed")
                    all_parsed_numerical_predictions.append(np.nan)
                    continue

                generated_ids_trimmed_viz = generated_ids_viz[:, input_ids_viz.shape[1]:]
                output_text_full_viz = processor.batch_decode(generated_ids_trimmed_viz, skip_special_tokens=True)[0]
                all_raw_text_outputs.append(output_text_full_viz)

                num_match_viz = re.findall(r"[-+]?\d*\.\d+|\d+", output_text_full_viz.split('\n')[0])
                if not num_match_viz: num_match_viz = re.findall(r"[-+]?\d*\.\d+|\d+", output_text_full_viz)
                if num_match_viz:
                    try: all_parsed_numerical_predictions.append(float(num_match_viz[0]))
                    except ValueError: all_parsed_numerical_predictions.append(np.nan)
                else: all_parsed_numerical_predictions.append(np.nan)
    # ...

    # =========================================================================
    # MODIFICATION START: Metrics Calculation (MSE, MAE, R2)
    # =========================================================================
    valid_predictions = [p for p in all_parsed_numerical_predictions if not np.isnan(p)]
    corresponding_true_labels = [all_true_labels[i] for i,p in enumerate(all_parsed_numerical_predictions) if not np.isnan(p)]

    metrics = {
        'mse': float('inf'),
        'mae': float('inf'),
        'r2': float('-inf')
    }

    if valid_predictions and corresponding_true_labels:
        # Calculate metrics
        metrics['mse'] = mean_squared_error(corresponding_true_labels, valid_predictions)
        metrics['mae'] = mean_absolute_error(corresponding_true_labels, valid_predictions)
        metrics['r2'] = r2_score(corresponding_true_labels, valid_predictions)

        # Print metrics
        print(f"\n--- {desc} Metrics ---")
        print(f"Total Samples Processed: {len(all_true_labels)}")
        print(f"Valid Samples for Metrics: {len(valid_predictions)}")
        print(f"MSE (Mean Squared Error):       {metrics['mse']:.4f}")
        print(f"MAE (Mean Absolute Error):      {metrics['mae']:.4f}")
        print(f"R2 (R-squared):                 {metrics['r2']:.4f}")
        print("--------------------------------" + "-"*len(desc))

    else:
        print(f"\n{desc}: Could not calculate metrics (no valid predictions).")
    # =========================================================================
    # MODIFICATION END
    # =========================================================================


    # Detailed Visualization
    # ... (原有代码)
    print(f"\n--- {desc} Detailed Results (with Images and Output) ---")
    num_samples_to_show_viz = min(len(all_true_labels), 51) # Show up to 51
    channel_names_viz = ["Cortical Thickness", "Cortical Curvature", "Myelination", "Sulcal Depth"]

    for idx in range(num_samples_to_show_viz):
        print("\n" + "="*80 + f"\nSample #{idx+1}" + "\n" + "-"*80)
        true_val_str = f"{all_true_labels[idx]:.1f}"
        pred_text_str = all_raw_text_outputs[idx] if idx < len(all_raw_text_outputs) else "N/A"
        parsed_val_str = "N/A"
        if idx < len(all_parsed_numerical_predictions) and not np.isnan(all_parsed_numerical_predictions[idx]):
            parsed_val_str = f"{all_parsed_numerical_predictions[idx]:.1f}"

        print(f"True Label         : {true_val_str}")
        print(f"Parsed Prediction  : {parsed_val_str}")
        print(f"Full Model Output  :\n{pred_text_str}")
        print("-" * 80)

        if idx < len(all_image_sets_for_viz) and all_image_sets_for_viz[idx]:
            fig, axes = plt.subplots(1, 4, figsize=(20, 5))
            fig.suptitle(f"Sample #{idx+1} - Input Images (True Age: {true_val_str})", fontsize=16)
            for channel_i, pil_img_viz in enumerate(all_image_sets_for_viz[idx]):
                axes[channel_i].imshow(pil_img_viz)
                axes[channel_i].set_title(channel_names_viz[channel_i])
                axes[channel_i].axis('off')
            plt.tight_layout(rect=[0, 0.03, 1, 0.95])
            plt.show()
        else: print("(Image data not available for this sample's display)")
        print("="*80)
    print(f"\n--- End of {desc} Detailed Results (Displayed {num_samples_to_show_viz} samples) ---")
    # ...

    model_to_validate.train()
    # Return the dictionary of metrics
    return metrics

# =========================================================================
# The function calls remain the same.
# If you want to use the returned metrics, you can assign them to a variable.
# Example:
# validation_results = validate_model_for_visualization(...)
# print(validation_results)
# =========================================================================

if 'peft_model' in locals() and peft_model is not None and peft_model.parameters().__next__().requires_grad : # If peft_model exists and was trained
    print("\nValidating the fine-tuned PEFT model (peft_model) with visualization...")
    # You can capture the results like this:
    # peft_metrics = validate_model_for_visualization(peft_model, qwen_processor, validation_loader, device)
    # print("Final PEFT Metrics:", peft_metrics)
    validate_model_for_visualization(peft_model, qwen_processor, validation_loader, device)
elif 'loaded_peft_model' in locals() and loaded_peft_model is not None:
    print("\nValidating the loaded PEFT model (loaded_peft_model) with visualization...")
    validate_model_for_visualization(loaded_peft_model, processor_loaded if 'processor_loaded' in locals() else qwen_processor, validation_loader, 'cuda:0')
else:
    print("\nNo fine-tuned or loaded PEFT model available for final visualization validation.")